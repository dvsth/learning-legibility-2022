{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "to the repo for\n",
    "\n",
    "*Learning the Legibility of Visual Text Perturbations* (EACL 2023)\n",
    "\n",
    "by Dev Seth, Rickard Stureborg, Danish Pruthi and Bhuwan Dhingra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A `LEGIT` Introduction\n",
    "This notebook provides a helpful starting point to interact with the datasets and models presented in the Learning Legibility paper.\n",
    "\n",
    "All assets are hosted on the HuggingFace Hub and can be used with the `transformers` and `datasets` libraries: \n",
    "  - TrOCR-MT Model: https://huggingface.co/dvsth/LEGIT-TrOCR-MT \n",
    "  - LEGIT Dataset: https://huggingface.co/datasets/dvsth/LEGIT\n",
    "  - Perturbed Jigsaw Dataset: https://huggingface.co/datasets/dvsth/LEGIT-VIPER-Jigsaw-Toxic-Comment-Perturbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external imports -- use pip or conda to install these packages\n",
    "import torch\n",
    "from transformers import TrOCRProcessor, AutoModel, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# local imports -- make sure these files are in the same directory as this notebook\n",
    "from LegibilityModel import LegibilityModel\n",
    "from Trainer import MultiTaskTrainer\n",
    "from Metrics import binary_classification_metric, ranking_metric\n",
    "from utils import Renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model schema and pretrained weights\n",
    "# (this may take some time to download)\n",
    "model = AutoModel.from_pretrained(\"dvsth/LEGIT-TrOCR-MT\", revision='main', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive dataset preview available [here](https://huggingface.co/datasets/dvsth/LEGIT/viewer/dvsth--LEGIT/test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('dvsth/LEGIT').with_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training/Eval Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor provides image normalization and resizing\n",
    "preprocessor = TrOCRProcessor.from_pretrained(\n",
    "    \"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# apply preprocessing batch-wise\n",
    "def collate_fn(data):\n",
    "    return {\n",
    "        'choice': torch.tensor([d['choice'].item() for d in data]),\n",
    "        'img0': preprocessor([d['img0'] for d in data], return_tensors='pt')['pixel_values'],\n",
    "        'img1': preprocessor([d['img1'] for d in data], return_tensors='pt')['pixel_values']\n",
    "    }\n",
    "\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=f'runs',             # change this to a unique path for each run, e.g. f'runs/{run_id}'\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,             # we found 3 epochs to be sufficient for convergence on the base models\n",
    "    per_device_train_batch_size=26, # fits on 1 x NVIDIA A6000, 48GB VRAM\n",
    "    per_device_eval_batch_size=26,  # can be increased to 32\n",
    "    gradient_accumulation_steps=2,  # increase this to fit on a smaller GPU\n",
    "    warmup_steps=0,             \n",
    "    weight_decay=0.0,\n",
    "    learning_rate=1e-5,             # we found this to be the best initial learning rate for the base models\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=50,\n",
    "    fp16=False,                     \n",
    "    load_best_model_at_end=True,    # load the best model at the end of training based on validation F1\n",
    "    metric_for_best_model='f1_score')\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    compute_metrics=binary_classification_metric, # check out metrics.py for a list of metrics\n",
    "    args=train_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['valid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate predictions and compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(dataset['test'].select(range(100))) # takes ~1-2 minutes on a laptop CPU\n",
    "print(predictions.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9a33fd02dcd74fd53701f10c0433ded41be0a0f53c9699722a73f690e69c2bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
