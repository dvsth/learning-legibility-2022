{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "to the repo for\n",
    "\n",
    "*Learning the Legibility of Visual Text Perturbations* (EACL 2023)\n",
    "\n",
    "by Dev Seth, Rickard Stureborg, Danish Pruthi and Bhuwan Dhingra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A `LEGIT` Introduction\n",
    "This notebook provides a helpful starting point to interact with the datasets and models presented in the Learning Legibility paper.\n",
    "\n",
    "All assets are hosted on the HuggingFace Hub and can be used with the `transformers` and `datasets` libraries: \n",
    "  - TrOCR-MT Model: https://huggingface.co/dvsth/LEGIT-TrOCR-MT \n",
    "  - LEGIT Dataset: https://huggingface.co/datasets/dvsth/LEGIT\n",
    "  - Perturbed Jigsaw Dataset: https://huggingface.co/datasets/dvsth/LEGIT-VIPER-Jigsaw-Toxic-Comment-Perturbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external imports -- use pip or conda to install these packages\n",
    "import torch\n",
    "from transformers import TrOCRProcessor, AutoModel, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# local imports\n",
    "from classes.LegibilityModel import LegibilityModel\n",
    "from classes.Trainer import MultiTaskTrainer\n",
    "from classes.Metrics import binary_classification_metric, ranking_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/dvsth/.cache/huggingface/hub/models--dvsth--LEGIT-TrOCR-MT/snapshots/eb8f84a70f3e400077663e48d497ab7ce796c724/config.json\n",
      "Model config VisionEncoderDecoderConfig {\n",
      "  \"_commit_hash\": \"eb8f84a70f3e400077663e48d497ab7ce796c724\",\n",
      "  \"_name_or_path\": \"dvsth/LEGIT-TrOCR-MT\",\n",
      "  \"architectures\": [\n",
      "    \"LegibilityModel\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoModel\": \"LegibilityModel.LegibilityModel\"\n",
      "  },\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"activation_dropout\": 0.0,\n",
      "    \"activation_function\": \"gelu\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 0,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": 0.0,\n",
      "    \"cross_attention_hidden_size\": 768,\n",
      "    \"d_model\": 1024,\n",
      "    \"decoder_attention_heads\": 16,\n",
      "    \"decoder_ffn_dim\": 4096,\n",
      "    \"decoder_layerdrop\": 0.0,\n",
      "    \"decoder_layers\": 12,\n",
      "    \"decoder_start_token_id\": 2,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"dropout\": 0.1,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"init_std\": 0.02,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layernorm_embedding\": true,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"trocr\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 1,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"scale_embedding\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.25.1\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": false,\n",
      "    \"use_learned_position_embeddings\": true,\n",
      "    \"vocab_size\": 50265\n",
      "  },\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_probs_dropout_prob\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"encoder_stride\": 16,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.0,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 384,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"vit\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 16,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"qkv_bias\": false,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.25.1\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false\n",
      "  },\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"model_type\": \"vision-encoder-decoder\",\n",
      "  \"processor_class\": \"TrOCRProcessor\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": null\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/dvsth/.cache/huggingface/hub/models--dvsth--LEGIT-TrOCR-MT/snapshots/eb8f84a70f3e400077663e48d497ab7ce796c724/pytorch_model.bin\n",
      "loading configuration file config.json from cache at /Users/dvsth/.cache/huggingface/hub/models--microsoft--trocr-base-handwritten/snapshots/0cc7abaad739c7902dabd562356c7e6a7be834ea/config.json\n",
      "Model config VisionEncoderDecoderConfig {\n",
      "  \"_commit_hash\": \"0cc7abaad739c7902dabd562356c7e6a7be834ea\",\n",
      "  \"_name_or_path\": \"microsoft/trocr-base-handwritten\",\n",
      "  \"architectures\": [\n",
      "    \"VisionEncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"activation_dropout\": 0.0,\n",
      "    \"activation_function\": \"gelu\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 0,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"classifier_dropout\": 0.0,\n",
      "    \"cross_attention_hidden_size\": 768,\n",
      "    \"d_model\": 1024,\n",
      "    \"decoder_attention_heads\": 16,\n",
      "    \"decoder_ffn_dim\": 4096,\n",
      "    \"decoder_layerdrop\": 0.0,\n",
      "    \"decoder_layers\": 12,\n",
      "    \"decoder_start_token_id\": 2,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"dropout\": 0.1,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"init_std\": 0.02,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layernorm_embedding\": true,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"trocr\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 1,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"scale_embedding\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.25.1\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": false,\n",
      "    \"use_learned_position_embeddings\": true,\n",
      "    \"vocab_size\": 50265\n",
      "  },\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_probs_dropout_prob\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"encoder_stride\": 16,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.0,\n",
      "    \"hidden_size\": 768,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 384,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3072,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"vit\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 16,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"qkv_bias\": false,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.25.1\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false\n",
      "  },\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"model_type\": \"vision-encoder-decoder\",\n",
      "  \"processor_class\": \"TrOCRProcessor\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": null\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing LegibilityModel.\n",
      "\n",
      "All the weights of LegibilityModel were initialized from the model checkpoint at dvsth/LEGIT-TrOCR-MT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LegibilityModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load the model schema and pretrained weights\n",
    "# (this may take some time to download)\n",
    "model = AutoModel.from_pretrained(\"dvsth/LEGIT-TrOCR-MT\", revision='main', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive dataset preview available [here](https://huggingface.co/datasets/dvsth/LEGIT/viewer/dvsth--LEGIT/test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dvsth--LEGIT-d84a4d72774d3652\n",
      "Found cached dataset parquet (/Users/dvsth/.cache/huggingface/datasets/dvsth___parquet/dvsth--LEGIT-d84a4d72774d3652/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448a02e6521b4f58bfb58614873f8ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('dvsth/LEGIT').with_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training/Eval Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /Users/dvsth/.cache/huggingface/hub/models--microsoft--trocr-base-handwritten/snapshots/0cc7abaad739c7902dabd562356c7e6a7be834ea/preprocessor_config.json\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "{param_name} should be a dictionary on of the following set of keys: {VALID_SIZE_DICT_KEYS}, got {size}. Converted to {size_dict}.\n",
      "Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 384,\n",
      "    \"width\": 384\n",
      "  }\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/dvsth/.cache/huggingface/hub/models--microsoft--trocr-base-handwritten/snapshots/0cc7abaad739c7902dabd562356c7e6a7be834ea/vocab.json\n",
      "loading file merges.txt from cache at /Users/dvsth/.cache/huggingface/hub/models--microsoft--trocr-base-handwritten/snapshots/0cc7abaad739c7902dabd562356c7e6a7be834ea/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/dvsth/.cache/huggingface/hub/models--microsoft--trocr-base-handwritten/snapshots/0cc7abaad739c7902dabd562356c7e6a7be834ea/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/dvsth/.cache/huggingface/hub/models--microsoft--trocr-base-handwritten/snapshots/0cc7abaad739c7902dabd562356c7e6a7be834ea/tokenizer_config.json\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# preprocessor provides image normalization and resizing\n",
    "preprocessor = TrOCRProcessor.from_pretrained(\n",
    "    \"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# apply preprocessing batch-wise\n",
    "def collate_fn(data):\n",
    "    return {\n",
    "        'choice': torch.tensor([d['choice'].item() for d in data]),\n",
    "        'img0': preprocessor([d['img0'] for d in data], return_tensors='pt')['pixel_values'],\n",
    "        'img1': preprocessor([d['img1'] for d in data], return_tensors='pt')['pixel_values']\n",
    "    }\n",
    "\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=f'runs',             # change this to a unique path for each run, e.g. f'runs/{run_id}'\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,             # we found 3 epochs to be sufficient for convergence on the base models\n",
    "    per_device_train_batch_size=26, # fits on 1 x NVIDIA A6000, 48GB VRAM\n",
    "    per_device_eval_batch_size=26,  # can be increased to 32\n",
    "    gradient_accumulation_steps=2,  # increase this to fit on a smaller GPU\n",
    "    warmup_steps=0,             \n",
    "    weight_decay=0.0,\n",
    "    learning_rate=1e-5,             # we found this to be the best initial learning rate for the base models\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=50,\n",
    "    fp16=False,                     \n",
    "    load_best_model_at_end=True,    # load the best model at the end of training based on validation F1\n",
    "    metric_for_best_model='f1_score')\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    compute_metrics=binary_classification_metric, # check out metrics.py for a list of metrics\n",
    "    args=train_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['valid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate predictions and compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `LegibilityModel.forward` and have been ignored: n1, word, k, k1, n, word1, model1, model0, word0. If n1, word, k, k1, n, word1, model1, model0, word0 are not expected by `LegibilityModel.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d545c18741d43dbbf4a44172f026c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.5344929695129395, 'test_precision': 0.9479166567925349, 'test_recall': 0.8921568539984622, 'test_accuracy': 0.8787878721303949, 'test_f1_score': 0.9191914103665608, 'test_runtime': 47.7405, 'test_samples_per_second': 2.095, 'test_steps_per_second': 0.084}\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(dataset['test'].select(range(100))) # takes ~1-2 minutes on a laptop CPU\n",
    "print(predictions.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9a33fd02dcd74fd53701f10c0433ded41be0a0f53c9699722a73f690e69c2bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
